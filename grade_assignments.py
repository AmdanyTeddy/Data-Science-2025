#!/usr/bin/env python3
"""Automated Assignment Grader for GitHub Class Repository

Handles Python files, Jupyter notebooks, and detects AI-generated content.
Refined for event-driven grading (e.g., on PR merge) by allowing grading for a specific student.
"""

import os
import json
import re
import ast
import nbformat
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import hashlib
import difflib
import statistics
from collections import defaultdict, Counter
import logging
import argparse # Added for argument parsing
import csv # Added for CSV writing

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeAnalyzer:
    """Analyzes code quality, documentation, and potential AI generation."""

    def __init__(self):
        self.patterns = {
            'ai_indicators': [
                r'\b(chatgpt|openai|gpt-3|gpt-4|copilot)\b',
                r'\b(as an ai|as a language model|as an llm)\b',
                r'\b(generated by|created by|powered by)\b.*\b(ai|artificial intelligence)\b'
            ],
            'complexity_indicators': [
                r'(for|while)\s*\([^)]*\)\s*\{[^}]*\}\s*else\s*\{[^}]*\}', # Complex nested if/else/loops (C-style, example)
                # Add more Python-specific complexity patterns if needed
            ]
        }
        # Common Python built-in functions and keywords for basic vocabulary check
        self.common_words = set(dir(__builtins__)) | {'if', 'else', 'for', 'while', 'def', 'class', 'import', 'from', 'as', 'with', 'try', 'except', 'finally', 'raise', 'assert', 'yield', 'lambda', 'and', 'or', 'not', 'in', 'is', 'None', 'True', 'False'}

    def analyze_python_file(self, file_path: str) -> Dict[str, Any]:
        """Analyze a Python file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code_content = f.read()
        except Exception as e:
            logger.error(f"Could not read {file_path}: {e}")
            return {}

        # Analyze the content directly
        analysis = self._analyze_code_content(code_content, file_path)
        analysis['file_type'] = 'Python'
        return analysis

    def analyze_jupyter_notebook(self, file_path: str) -> Dict[str, Any]:
        """Analyze a Jupyter notebook"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                nb = nbformat.read(f, as_version=4)
        except Exception as e:
            logger.error(f"Could not read {file_path}: {e}")
            return {}

        total_code_lines = 0
        total_docstring_lines = 0
        all_syntax_errors = []
        all_style_issues = []
        all_ai_detections = []
        complexity_scores = []
        all_cell_analyses = []

        for i, cell in enumerate(nb.cells):
            if cell.cell_type == 'code':
                code_content = cell.source
                if not code_content.strip(): # Skip empty code cells
                    continue
                total_code_lines += len(code_content.splitlines())

                # Analyze the code content of the cell directly
                cell_identifier = f"{file_path}#cell-{i}"
                cell_analysis = self._analyze_code_content(code_content, cell_identifier)
                all_cell_analyses.append(cell_analysis)

                # Aggregate results from the cell analysis
                total_docstring_lines += cell_analysis.get('docstring_lines', 0)
                all_syntax_errors.extend(cell_analysis.get('syntax_errors', []))
                all_style_issues.extend(cell_analysis.get('style_issues', []))
                all_ai_detections.append(cell_analysis.get('ai_detection', {}))
                complexity_scores.append(cell_analysis.get('complexity_score', 0))


        # --- Aggregate Results Across Cells ---

        # Aggregate AI detection results from cells
        # Simple approach: count high likelihood detections
        high_ai_cells = sum(1 for detection in all_ai_detections if detection.get('likelihood') in ['High', 'Very High'])
        medium_ai_cells = sum(1 for detection in all_ai_detections if detection.get('likelihood') == 'Medium')

        # Determine overall likelihood based on cell results
        if high_ai_cells > 1: # Arbitrary threshold
            overall_likelihood = 'High'
            overall_explanation = f"Multiple cells ({high_ai_cells}) show high likelihood of AI generation."
        elif high_ai_cells == 1:
            overall_likelihood = 'High' # Or Medium, depending on strictness
            overall_explanation = f"One cell shows high likelihood of AI generation."
        elif medium_ai_cells > 2:
             overall_likelihood = 'Medium'
             overall_explanation = f"Several cells ({medium_ai_cells}) show medium likelihood."
        elif medium_ai_cells >= 1:
             overall_likelihood = 'Medium'
             overall_explanation = f"One or two cells ({medium_ai_cells}) show medium likelihood."
        else:
            overall_likelihood = 'Low' # Default if no strong indicators
            overall_explanation = "No strong AI indicators found in cells."

        overall_ai_detection = {
            'patterns_found': any(d.get('patterns_found', False) for d in all_ai_detections),
            'explanation': overall_explanation,
            'likelihood': overall_likelihood
        }

        # Calculate average complexity score
        avg_complexity_score = sum(complexity_scores) / len(complexity_scores) if complexity_scores else 0

        analysis = {
            'file_type': 'Jupyter Notebook',
            'lines_of_code': total_code_lines,
            'docstring_lines': total_docstring_lines,
            'syntax_errors': all_syntax_errors,
            'style_issues': all_style_issues,
            'ai_detection': overall_ai_detection,
            'complexity_score': avg_complexity_score,
            # Optional: Add detailed cell analysis if needed
            # 'cell_analyses': all_cell_analyses
        }
        return analysis

    def _analyze_code_content(self, code_content: str, source_identifier: str) -> Dict[str, Any]:
        """
        Analyze the content of code (from a .py file or a notebook cell).
        source_identifier is for logging/context (e.g., filename or cell reference)
        """
        analysis = {
            'lines_of_code': len(code_content.splitlines()),
            'docstring_lines': self._count_docstrings(code_content),
            'syntax_errors': self._check_syntax(code_content),
            'style_issues': self._check_style(code_content),
            'ai_detection': self._detect_ai_patterns(code_content),
            'complexity_score': self._assess_complexity(code_content)
        }
        return analysis

    def _count_docstrings(self, code: str) -> int:
        """Count lines in docstrings"""
        try:
            tree = ast.parse(code)
        except SyntaxError:
            return 0 # Cannot count if syntax is broken

        documented = 0
        for node in ast.walk(tree):
            # Check for function or class docstrings (first statement being a string)
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):
                 # Safely get docstring, handle cases where it might be None
                 docstring = ast.get_docstring(node, clean=False)
                 if docstring:
                    documented += len(docstring.splitlines())
                 # Alternative check (older Python compatibility or edge cases)
                 # elif (node.body and isinstance(node.body[0], ast.Expr) and
                 #     isinstance(node.body[0].value, (ast.Constant, ast.Str))):
                 #    # This is less reliable for counting lines, ast.get_docstring is preferred
                 #    documented += 1 # Count the statement itself, not lines
        return documented

    def _check_syntax(self, code: str) -> List[str]:
        """Check for syntax errors"""
        errors = []
        try:
            ast.parse(code)
        except SyntaxError as e:
            errors.append(f"Syntax error at line {getattr(e, 'lineno', 'unknown')}: {e.msg}")
        except Exception as e:
            errors.append(f"Parse error: {str(e)}")
        return errors

    def _check_style(self, code: str) -> List[str]:
        """Basic style checking"""
        issues = []
        lines = code.splitlines()
        for i, line in enumerate(lines, 1):
            if len(line) > 100:  # Line too long
                issues.append(f"Line {i} exceeds 100 characters")
            if '\t' in line:  # Tabs used for indentation
                issues.append(f"Line {i} uses tab for indentation")
        return issues

    def _detect_ai_patterns(self, content: str) -> Dict[str, Any]:
        """Detect potential AI-generated content patterns"""
        content_lower = content.lower()
        found_patterns = []
        for pattern in self.patterns['ai_indicators']:
            if re.search(pattern, content_lower):
                found_patterns.append(pattern)

        # Simple heuristic: Check for unusually high ratio of common words (might indicate template/generic text)
        words = re.findall(r'\b\w+\b', content_lower)
        if words:
            common_word_ratio = len([w for w in words if w in self.common_words]) / len(words)
            if common_word_ratio > 0.9: # Arbitrary high threshold
                 found_patterns.append("Unusually high ratio of common programming words (potential template)")

        likelihood = 'Very Low'
        explanation = "No strong indicators of AI generation found."

        if found_patterns:
            # Basic scoring based on number of patterns found
            if len(found_patterns) >= 3:
                likelihood = 'High'
                explanation = f"Multiple AI indicators detected: {', '.join(found_patterns[:3])}..."
            elif len(found_patterns) >= 1:
                likelihood = 'Medium'
                explanation = f"Some AI indicators detected: {', '.join(found_patterns)}"

        return {
            'patterns_found': bool(found_patterns),
            'patterns': found_patterns,
            'explanation': explanation,
            'likelihood': likelihood
        }

    def _assess_complexity(self, code: str) -> float:
        """Basic complexity assessment (placeholder)"""
        # A very simple metric: ratio of non-empty lines to total lines
        lines = code.splitlines()
        non_empty_lines = [line for line in lines if line.strip()]
        if not lines:
            return 0.0
        return len(non_empty_lines) / len(lines) if len(lines) > 0 else 0.0


class AssignmentGrader:
    """Main grader class"""

    def __init__(self, repo_path: str = '.', target_student: Optional[str] = None):
        self.repo_path = Path(repo_path).resolve()
        self.submissions_path = self.repo_path / "Submissions"
        self.assignments_path = self.repo_path / "Assignments" # Might be used if comparing against templates
        self.analyzer = CodeAnalyzer()
        self.results = {}
        self.target_student = target_student # Store the target student name

    def grade_all_assignments(self) -> Dict[str, Any]:
        """Grade all student submissions (or only the target student if specified)"""
        logger.info("Starting assignment grading process...")
        if self.target_student:
            logger.info(f"Grading restricted to student: {self.target_student}")

        # Grade individual assignments
        individual_path = self.submissions_path / "assignments"
        if individual_path.exists():
            self._grade_directory(individual_path, 'individual')

        # Grade group work
        group_path = self.submissions_path / "groupwork"
        if group_path.exists():
            self._grade_directory(group_path, 'group')

        # Grade projects
        project_path = self.submissions_path / "projects"
        if project_path.exists():
            self._grade_directory(project_path, 'project')

        # Generate summary statistics (overall, not used for dashboard log which is per-student-event)
        self._generate_summary()
        return self.results

    def _grade_directory(self, directory: Path, assignment_type: str):
        """Grade submissions in a directory"""
        logger.info(f"Grading {assignment_type} submissions in {directory}")

        # If a target student is specified, only process that student's directory
        if self.target_student:
            student_dir = directory / self.target_student
            if student_dir.is_dir():
                logger.info(f"Grading {self.target_student}'s submissions in {assignment_type}...")
                student_results = self._grade_student_submissions(student_dir, self.target_student)
                if self.target_student not in self.results:
                    self.results[self.target_student] = {}
                self.results[self.target_student][assignment_type] = student_results
            else:
                logger.info(f"Target student '{self.target_student}' directory not found in {directory}. Skipping.")
        else:
            # Original logic: grade all students
            for student_dir in directory.iterdir():
                if student_dir.is_dir():
                    student_name = student_dir.name
                    logger.info(f"Grading {student_name}'s submissions in {assignment_type}...")
                    student_results = self._grade_student_submissions(student_dir, student_name)
                    if student_name not in self.results:
                        self.results[student_name] = {}
                    self.results[student_name][assignment_type] = student_results

    def _grade_student_submissions(self, student_dir: Path, student_name: str) -> Dict[str, Any]:
        """Grade all submissions for a student"""
        submissions = []
        for file_path in student_dir.rglob("*"):
            if file_path.is_file() and self._is_gradeable_file(file_path):
                logger.info(f"Analyzing {file_path}")
                if file_path.suffix == '.py':
                    analysis = self.analyzer.analyze_python_file(str(file_path))
                elif file_path.suffix == '.ipynb':
                    analysis = self.analyzer.analyze_jupyter_notebook(str(file_path))
                else:
                    continue # Should not happen due to _is_gradeable_file check, but safe

                # Generate grade and feedback
                grade_info = self._calculate_grade(analysis)
                submission = {
                    'file_name': file_path.name,
                    'file_path': str(file_path.relative_to(self.repo_path)),
                    'analysis': analysis,
                    'grade': grade_info['grade'],
                    'feedback': grade_info['feedback'],
                    'ai_detection': analysis.get('ai_detection', {})
                }
                submissions.append(submission)

        # Aggregate results for this student and assignment type
        total_submissions = len(submissions)
        grades = [s['grade'] for s in submissions if isinstance(s['grade'], (int, float))]
        average_grade = round(statistics.mean(grades), 2) if grades else 0
        ai_flagged = sum(1 for s in submissions if s.get('ai_detection', {}).get('likelihood') in ['High', 'Very High'])

        return {
            'total_submissions': total_submissions,
            'average_grade': average_grade,
            'ai_flagged': ai_flagged,
            'submissions': submissions
        }

    def _is_gradeable_file(self, file_path: Path) -> bool:
        """Check if a file should be graded"""
        return file_path.suffix in ['.py', '.ipynb'] and not file_path.name.startswith('.')

    def _calculate_grade(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate grade and generate feedback based on analysis"""
        if not analysis:
            return {'grade': 0, 'feedback': "File could not be analyzed."}

        score = 100.0
        feedback = []

        # Deduct points for syntax errors
        syntax_errors = analysis.get('syntax_errors', [])
        if syntax_errors:
            deduction = min(len(syntax_errors) * 10, 50) # Max 50 points off
            score -= deduction
            feedback.append(f"Syntax Errors ({len(syntax_errors)}): -{deduction} points")

        # Deduct points for style issues
        style_issues = analysis.get('style_issues', [])
        if style_issues:
            deduction = min(len(style_issues) * 2, 20) # Max 20 points off
            score -= deduction
            feedback.append(f"Style Issues ({len(style_issues)}): -{deduction} points")

        # Deduct points for low documentation
        lines_of_code = analysis.get('lines_of_code', 0)
        docstring_lines = analysis.get('docstring_lines', 0)
        if lines_of_code > 10: # Only check if there's substantial code
            doc_ratio = docstring_lines / lines_of_code if lines_of_code > 0 else 0
            if doc_ratio < 0.1: # Less than 10% documentation
                deduction = max(0, (0.1 - doc_ratio) * 100 * 5) # Scale deduction
                score -= min(deduction, 15) # Max 15 points off
                feedback.append(f"Low Documentation: -{min(deduction, 15):.1f} points")

        # Deduct points for potential AI use (high likelihood)
        ai_detection = analysis.get('ai_detection', {})
        ai_likelihood = ai_detection.get('likelihood', 'Very Low')
        if ai_likelihood == 'Very High':
            score -= 30
            feedback.append("Potential AI Use (Very High Likelihood): -30 points")
        elif ai_likelihood == 'High':
            score -= 20
            feedback.append("Potential AI Use (High Likelihood): -20 points")
        elif ai_likelihood == 'Medium':
             score -= 10
             feedback.append("Potential AI Use (Medium Likelihood): -10 points")

        # Ensure score is within bounds
        score = max(0, min(100, score))

        return {
            'grade': round(score, 2),
            'feedback': '; '.join(feedback) if feedback else "Good work!"
        }

    def _generate_summary(self):
        """Generate overall summary statistics"""
        all_grades = []
        ai_flagged_count = 0
        total_submissions = 0
        total_students = 0

        for student_name, student_data in self.results.items():
            if student_name == '_summary':
                continue
            total_students += 1
            for assignment_type, type_data in student_data.items():
                submissions = type_data.get('submissions', [])
                for submission in submissions:
                    total_submissions += 1
                    if isinstance(submission['grade'], (int, float)):
                        all_grades.append(submission['grade'])
                    ai_likelihood = submission['ai_detection'].get('likelihood', 'Very Low')
                    if ai_likelihood in ['High', 'Very High']:
                        ai_flagged_count += 1

        summary = {
            'total_students': total_students,
            'total_submissions': total_submissions,
            'average_grade': round(statistics.mean(all_grades), 2) if all_grades else 0,
            'grade_distribution': {
                'A (90-100)': len([g for g in all_grades if g >= 90]),
                'B (80-89)': len([g for g in all_grades if 80 <= g < 90]),
                'C (70-79)': len([g for g in all_grades if 70 <= g < 80]),
                'D (60-69)': len([g for g in all_grades if 60 <= g < 70]),
                'F (0-59)': len([g for g in all_grades if g < 60])
            },
            'ai_flagged_submissions': ai_flagged_count,
            'ai_flagged_percentage': round((ai_flagged_count / max(total_submissions, 1)) * 100, 2)
        }
        self.results['_summary'] = summary
        logger.info(f"Grading complete: {total_students} students, {total_submissions} submissions")

    # --- NEW METHOD: Append to Dashboard Log ---
    def _append_to_dashboard_log(self, output_csv_path='dashboard_grading_history.csv'):
        """
        Appends summary data for the target student to a CSV log file for the dashboard.
        This is called only if a specific student was graded (--student argument).
        """
        # Check if the method should run
        if not self.target_student or self.target_student not in self.results:
            logger.debug("No target student specified or student not found in results. Skipping dashboard log append.")
            return

        student_data = self.results[self.target_student]
        # Note: self.results['_summary'] might not be fully populated if only one student was graded.
        # We calculate summary stats directly from the student's data.

        # Prepare data for a single row representing this grading event for this student
        timestamp = datetime.now().isoformat() + 'Z' # UTC ISO format
        student_name = self.target_student

        # Calculate overall metrics for this student from their specific results
        # We need to aggregate across potentially multiple assignment types (individual, project, etc.)
        total_submissions = 0
        sum_of_weighted_averages = 0.0 # Sum of (average_grade * total_submissions) for each type
        ai_flags = 0

        # Iterate through the assignment types graded for this specific student (e.g., 'individual', 'project')
        for assignment_type, type_data in student_data.items():
            if isinstance(type_data, dict): # Ensure type_data is a dict (not _summary etc.)
                total_submissions_for_type = type_data.get('total_submissions', 0)
                average_grade_for_type = type_data.get('average_grade', 0)
                ai_flags_for_type = type_data.get('ai_flagged', 0)

                total_submissions += total_submissions_for_type
                # Weight the average grade by the number of submissions for that type
                sum_of_weighted_averages += average_grade_for_type * total_submissions_for_type
                ai_flags += ai_flags_for_type

        # Avoid division by zero if no submissions were found/graded for the student
        if total_submissions > 0 and sum_of_weighted_averages > 0:
            # Calculate the overall average grade across all submissions for this student in this run
            overall_average_grade = round(sum_of_weighted_averages / total_submissions, 2)
        else:
            # Handle case where student directory exists but no gradeable files were found/graded
            # Or if averages were 0. Using 0 is reasonable here.
            overall_average_grade = 0

        # Data row to append
        log_data = {
            'timestamp': timestamp,
            'student_name': student_name,
            'average_grade': overall_average_grade,
            'ai_flags': ai_flags,
            # You can add more fields here if needed by the dashboard in the future
            # e.g., 'total_submissions': total_submissions
        }

        file_exists = os.path.isfile(output_csv_path)

        try:
            # Open the file in append mode
            with open(output_csv_path, mode='a', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['timestamp', 'student_name', 'average_grade', 'ai_flags'] # Define column order
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                # Write header only if the file is new/doesn't exist yet
                if not file_exists:
                    writer.writeheader()

                # Write the row for this student's grading event
                writer.writerow(log_data)
            logger.info(f"Appended summary data for {student_name} to {output_csv_path}")

        except Exception as e:
            # Log any errors that occur during file writing
            logger.error(f"Failed to append data to {output_csv_path}: {e}")
    # --- END NEW METHOD ---

    def generate_feedback_files(self):
        """Generate individual feedback files for each student (or only the target student)"""
        feedback_dir = self.repo_path / "feedback"
        feedback_dir.mkdir(exist_ok=True)

        students_to_process = [self.target_student] if self.target_student else self.results.keys()

        for student_name in students_to_process:
            if student_name == '_summary':
                continue

            # Get student data, potentially from self.results if already graded, or re-grade if needed
            student_data = self.results.get(student_name)
            if not student_data:
                # This case might occur if the script is run targeting a student not yet in results.
                # Re-grade just this student.
                logger.info(f"Re-grading {student_name} for feedback generation as they are not in current results.")
                # Find student directory (assuming standard structure)
                student_dirs = list(self.submissions_path.rglob(f"{student_name}/"))
                if student_dirs:
                    student_dir = student_dirs[0] # Take the first match
                    # Determine assignment type from path (simplified)
                    # This is a bit fragile, better to iterate through known types
                    assignment_type = student_dir.parent.name # e.g., 'assignments', 'projects'
                    if assignment_type in ['assignments', 'groupwork', 'projects']:
                        student_results = self._grade_student_submissions(student_dir, student_name)
                        # Structure it correctly for _write_student_feedback
                        student_data = {assignment_type: student_results}
                    else:
                        logger.warning(f"Unknown assignment type directory for {student_name}: {assignment_type}")
                        continue
                else:
                    logger.warning(f"Could not find directory for student {student_name} for feedback generation.")
                    continue # Skip if directory not found

            # Write feedback file for the student
            feedback_file = feedback_dir / f"{student_name}_feedback.md"
            self._write_student_feedback(feedback_file, student_name, student_data)

    def _write_student_feedback(self, file_path: Path, student_name: str, student_data: Dict):
        """Write feedback file for a student"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"# Feedback for {student_name}\n\n")
                f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                # Iterate through the assignment types present in student_data
                # (e.g., 'individual', 'group', 'project')
                for assignment_type, type_data in student_data.items():
                    if not isinstance(type_data, dict): # Skip '_summary' or other non-dict entries
                         continue
                    f.write(f"## {assignment_type.title()} Assignments\n\n")
                    f.write(f"**Average Grade:** {type_data.get('average_grade', 'N/A')}/100\n\n")
                    f.write(f"**Total Submissions:** {type_data.get('total_submissions', 0)}\n\n")
                    if type_data.get('ai_flagged', 0) > 0:
                        f.write(f"**⚠️ AI Detection Flags:** {type_data['ai_flagged']} submissions\n\n")

                    f.write("### Individual Submissions:\n\n")
                    for submission in type_data.get('submissions', []):
                        f.write(f"#### {submission['file_name']}\n\n")
                        f.write(f"**Grade:** {submission['grade']}/100\n\n")
                        f.write(f"**Feedback:** {submission['feedback']}\n\n")
                        ai_info = submission.get('ai_detection', {})
                        if ai_info.get('patterns_found'):
                            f.write(f"**AI Detection:** {ai_info.get('explanation', 'Potential AI use detected.')}\n\n")
                        f.write("---\n\n")
            logger.info(f"Feedback written to {file_path}")

        except Exception as e:
            logger.error(f"Failed to write feedback file {file_path}: {e}")


def main():
    """Main function to run the grader"""
    parser = argparse.ArgumentParser(description='Automated Assignment Grader')
    parser.add_argument('--repo-path', default='.', help='Path to the repository')
    parser.add_argument('--output', default='grading_results.json', help='Output file for results')
    parser.add_argument('--generate-feedback', action='store_true', help='Generate individual feedback files')
    parser.add_argument('--student', type=str, help='Grade only submissions for this specific student (directory name)')
    # - Potentially add an argument to control dashboard logging -
    # parser.add_argument('--no-dashboard-log', action='store_true', help='Skip appending to dashboard log')
    args = parser.parse_args()

    # Initialize grader with optional target student
    grader = AssignmentGrader(args.repo_path, target_student=args.student)
    # Grade assignments (all or specific student)
    results = grader.grade_all_assignments()
    # Save results
    output_file = Path(args.output)
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"Results saved to: {output_file}")
    except Exception as e:
        logger.error(f"Failed to save results to {output_file}: {e}")
        return # Exit if saving results fails

    # --- NEW: Append to dashboard log if a specific student was graded ---
    # if args.student and not args.no_dashboard_log: # Use flag if added
    if args.student: # Only log if a specific student was targeted
        grader._append_to_dashboard_log() # Call the new method
    # --- END NEW ---

    # Generate feedback files if requested (for all or specific student)
    if args.generate_feedback:
        grader.generate_feedback_files()
        logger.info("Feedback files generated in 'feedback' directory")

    # Print summary
    summary = results.get('_summary', {})
    print(f"\n{'='*50}")
    print("GRADING SUMMARY")
    print(f"{'='*50}")
    print(f"Total Students: {summary.get('total_students', 0)}")
    print(f"Total Submissions: {summary.get('total_submissions', 0)}")
    print(f"Average Grade: {summary.get('average_grade', 0)}/100")
    print(f"AI Flagged: {summary.get('ai_flagged_submissions', 0)} ({summary.get('ai_flagged_percentage', 0)}%)")
    print(f"\nGrade Distribution:")
    for grade_range, count in summary.get('grade_distribution', {}).items():
        print(f"  {grade_range}: {count} students")
    print(f"\nResults saved to: {args.output}")
    if args.generate_feedback:
         print("Feedback files generated.")
    if args.student:
        print(f"Grading was restricted to student: {args.student}")
        # - Optional: Print confirmation for dashboard log -
        # print(f"Dashboard summary appended for: {args.student}")

if __name__ == "__main__":
    main()
