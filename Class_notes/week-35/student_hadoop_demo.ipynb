{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6032f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Hadoop & MapReduce Student Hands-On Demo Script\n",
    "Module 10: Scalable Computing - From Single Machine to Distributed Processing\n",
    "\n",
    "This script demonstrates MapReduce concepts using Python and PySpark,\n",
    "bridging the gap between the Java lab and practical data science workflows.\n",
    "\n",
    "Usage:\n",
    "    python student_hadoop_demo.py\n",
    "\n",
    "Prerequisites:\n",
    "    pip install pyspark pandas numpy matplotlib seaborn\n",
    "\n",
    "Terminal commands to run before starting:\n",
    "    # 1. Check Python version\n",
    "    python --version\n",
    "\n",
    "    # 2. Install required packages\n",
    "    pip install pyspark pandas numpy matplotlib seaborn\n",
    "\n",
    "    # 3. Download Titanic dataset\n",
    "    curl -o titanic.csv https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
    "\n",
    "    # 4. Verify Java installation (required for PySpark)\n",
    "    java -version\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(title: str):\n",
    "    \"\"\"Print a formatted header\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\ud83d\ude80 {title}\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Let's make sure all packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_environment():\n",
    "    \"\"\"Check if all required packages are available\"\"\"\n",
    "    print_header(\"Environment Check\")\n",
    "\n",
    "    # Check Python version\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "\n",
    "    # Test basic imports\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        print(\"\u2705 Core data science libraries imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"\u274c Import error: {e}\")\n",
    "        print(\"Run: pip install pandas numpy matplotlib seaborn\")\n",
    "        return False\n",
    "\n",
    "    # Test PySpark\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        print(\"\u2705 PySpark imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"\u274c PySpark import error: {e}\")\n",
    "        print(\"Run: pip install pyspark\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\ud83c\udfaf Ready to start the Hadoop demo!\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We'll use Titanic data to demonstrate concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic_data() -> pd.DataFrame:\n",
    "    \"\"\"Load Titanic dataset with fallback to generated data\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('titanic.csv')\n",
    "        print(f\"\u2705 Titanic dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        print(\".2f\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"\u274c titanic.csv not found. Download it first:\")\n",
    "        print(\"curl -o titanic.csv https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "        print(\"\ud83d\udcdd Using generated sample data for demo\")\n",
    "\n",
    "        # Create sample data for demo\n",
    "        np.random.seed(42)\n",
    "        df = pd.DataFrame({\n",
    "            'PassengerId': range(1, 892),\n",
    "            'Survived': np.random.choice([0, 1], 891),\n",
    "            'Pclass': np.random.choice([1, 2, 3], 891),\n",
    "            'Name': [f'Passenger_{i}' for i in range(891)],\n",
    "            'Sex': np.random.choice(['male', 'female'], 891),\n",
    "            'Age': np.random.normal(30, 15, 891).clip(0, 80),\n",
    "            'Fare': np.random.exponential(30, 891)\n",
    "        })\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Distributed Computing?\n",
    "\n",
    "Single computers can't handle huge datasets. We need to distribute work across multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_single_machine_limits(df: pd.DataFrame):\n",
    "    \"\"\"Show why single machines can't handle big data\"\"\"\n",
    "    print_header(\"Part 1: Single Machine Limitations\")\n",
    "\n",
    "    # Show basic analysis\n",
    "    print(\"\ud83d\udccb Basic Analysis:\")\n",
    "    survival_rate = df['Survived'].mean()\n",
    "    print(\".1%\")\n",
    "\n",
    "    # Survival by class\n",
    "    survival_by_class = df.groupby('Pclass')['Survived'].mean()\n",
    "    print(\"\\n\ud83c\udfaf Survival by Passenger Class:\")\n",
    "    for pclass, rate in survival_by_class.items():\n",
    "        print(\".1%\")\n",
    "\n",
    "    # Simulate \"big data\" problem\n",
    "    print(\"\\n\ud83d\udcad What if we had 1 BILLION passengers?\")\n",
    "    big_data_rows = 1_000_000_000\n",
    "    estimated_memory = (df.memory_usage(deep=True).sum() / len(df)) * big_data_rows / 1024 / 1024 / 1024\n",
    "    print(\".1f\")\n",
    "    print(\"   Processing time would be impractical!\")\n",
    "    print(\"   \u2192 This is why we need distributed computing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Manual MapReduce\n",
    "\n",
    "MapReduce has 3 phases:\n",
    "1. **Map**: Extract key-value pairs\n",
    "2. **Shuffle**: Group by key\n",
    "3. **Reduce**: Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_mapreduce_demo(df: pd.DataFrame) -> Dict[int, Tuple[int, float, int, int]]:\n",
    "    \"\"\"Demonstrate manual MapReduce implementation\"\"\"\n",
    "    print_header(\"Part 2: Manual MapReduce Implementation\")\n",
    "\n",
    "    # MAP Phase: Extract survival data by class\n",
    "    def mapper_survival_by_class(record) -> Tuple[int, int]:\n",
    "        \"\"\"Map function: extract (pclass, survived) pairs\"\"\"\n",
    "        pclass = record['Pclass']\n",
    "        survived = record['Survived']\n",
    "        return (pclass, survived)\n",
    "\n",
    "    print(\"\ud83d\udd39 MAP PHASE: Extracting key-value pairs...\")\n",
    "    mapped_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        mapped_data.append(mapper_survival_by_class(row))\n",
    "\n",
    "    print(f\"   Generated {len(mapped_data)} key-value pairs\")\n",
    "    print(f\"   Sample pairs: {mapped_data[:5]}\")\n",
    "\n",
    "    # SHUFFLE Phase: Group by key\n",
    "    def shuffle_group_by_key(mapped_data: List[Tuple[int, int]]) -> Dict[int, List[int]]:\n",
    "        \"\"\"Shuffle function: group values by key\"\"\"\n",
    "        shuffled = {}\n",
    "        for key, value in mapped_data:\n",
    "            if key not in shuffled:\n",
    "                shuffled[key] = []\n",
    "            shuffled[key].append(value)\n",
    "        return shuffled\n",
    "\n",
    "    print(\"\\n\ud83d\udd00 SHUFFLE PHASE: Grouping by passenger class...\")\n",
    "    shuffled_data = shuffle_group_by_key(mapped_data)\n",
    "    for key, values in shuffled_data.items():\n",
    "        print(f\"   Class {key}: {len(values)} passengers, survival indicators: {values[:5]}...\")\n",
    "\n",
    "    # REDUCE Phase: Calculate survival rates\n",
    "    def reducer_survival_rate(key: int, values: List[int]) -> Tuple[int, float, int, int]:\n",
    "        \"\"\"Reduce function: calculate survival statistics\"\"\"\n",
    "        total_passengers = len(values)\n",
    "        survived_count = sum(values)\n",
    "        survival_rate = survived_count / total_passengers\n",
    "        return (key, survival_rate, survived_count, total_passengers)\n",
    "\n",
    "    print(\"\\n\u26a1 REDUCE PHASE: Calculating survival rates...\")\n",
    "    final_results = {}\n",
    "    for key, values in shuffled_data.items():\n",
    "        result = reducer_survival_rate(key, values)\n",
    "        final_results[key] = result\n",
    "        pclass, rate, survived, total = result\n",
    "        print(\".1%\")\n",
    "\n",
    "    print(\"\\n\ud83c\udf89 Manual MapReduce completed!\")\n",
    "    print(\"\ud83d\udca1 Key insight: Each phase can run in parallel across different machines!\")\n",
    "\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PySpark\n",
    "\n",
    "Spark makes MapReduce much easier with high-level APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_demo(df: pd.DataFrame, manual_results: Dict[int, Tuple[int, float, int, int]]):\n",
    "    \"\"\"Demonstrate PySpark MapReduce implementation\"\"\"\n",
    "    print_header(\"Part 3: PySpark MapReduce Implementation\")\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Initialize Spark Session\n",
    "    print(\"\ud83d\ude80 Initializing Spark Session...\")\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"TitanicMapReduceDemo\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.host\", \"localhost\") \\\n",
    "            .config(\"spark.ui.enabled\", \"false\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Spark initialization failed: {e}\")\n",
    "        print(\"\ud83d\udca1 This is expected in some environments. Let's continue with pandas-only demo.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\u2705 Spark {spark.version} session created\")\n",
    "\n",
    "    # Load data into Spark DataFrame\n",
    "    print(\"\\n\ud83d\udce5 Loading Titanic data into Spark...\")\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    print(f\"\u2705 Spark DataFrame created: {spark_df.count()} rows\")\n",
    "\n",
    "    # Show schema\n",
    "    print(\"\\n\ud83d\udccb Data Schema:\")\n",
    "    spark_df.printSchema()\n",
    "\n",
    "    # Spark SQL approach (equivalent to HiveQL)\n",
    "    print(\"\\n\ud83d\udd0d Spark SQL Query (equivalent to HiveQL):\")\n",
    "    spark_df.createOrReplaceTempView(\"titanic\")\n",
    "\n",
    "    sql_result = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            Pclass,\n",
    "            AVG(Survived) as survival_rate,\n",
    "            COUNT(*) as total_passengers,\n",
    "            SUM(Survived) as survived_count\n",
    "        FROM titanic\n",
    "        GROUP BY Pclass\n",
    "        ORDER BY Pclass\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\ud83d\udcca Results from Spark SQL:\")\n",
    "    sql_result.show()\n",
    "\n",
    "    # Convert to Pandas for comparison\n",
    "    spark_results = sql_result.toPandas()\n",
    "    print(\"\\n\ud83d\udd04 Comparison with our manual MapReduce:\")\n",
    "    for _, row in spark_results.iterrows():\n",
    "        manual_result = manual_results.get(row['Pclass'], (0, 0, 0, 0))\n",
    "        manual_rate = manual_result[1]\n",
    "        spark_rate = row['survival_rate']\n",
    "        diff = abs(manual_rate - spark_rate)\n",
    "        print(\".3%\")\n",
    "\n",
    "    print(\"\\n\ud83c\udfaf Spark automatically handles the MapReduce complexity!\")\n",
    "    print(\"\ud83d\udca1 This is how Hadoop/Hive works under the hood.\")\n",
    "\n",
    "    # Cleanup\n",
    "    spark.stop()\n",
    "    print(\"\\n\ud83e\uddf9 Spark session stopped.\")\n",
    "\n",
    "    return spark_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "\n",
    "Initialize Spark to start processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison(df: pd.DataFrame):\n",
    "    \"\"\"Compare performance between pandas and Spark\"\"\"\n",
    "    print_header(\"Part 4: Performance Analysis\")\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Initialize Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PerformanceTest\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Test 1: Pandas performance\n",
    "    print(\"\ud83d\udc3c Testing Pandas Performance...\")\n",
    "    start_time = time.time()\n",
    "    pandas_result = df.groupby('Pclass')['Survived'].agg(['mean', 'count', 'sum'])\n",
    "    pandas_time = time.time() - start_time\n",
    "    print(\".4f\")\n",
    "\n",
    "    # Test 2: Spark performance\n",
    "    print(\"\\n\u26a1 Testing Spark Performance...\")\n",
    "    start_time = time.time()\n",
    "    spark_result = spark_df.groupBy(\"Pclass\") \\\n",
    "        .agg({\"Survived\": \"avg\", \"PassengerId\": \"count\"}) \\\n",
    "        .withColumnRenamed(\"avg(Survived)\", \"survival_rate\") \\\n",
    "        .withColumnRenamed(\"count(PassengerId)\", \"total_passengers\")\n",
    "    spark_time = time.time() - start_time\n",
    "    print(\".4f\")\n",
    "\n",
    "    # Performance comparison\n",
    "    speedup = pandas_time / spark_time if spark_time > 0 else float('inf')\n",
    "    print(\"\\n\ud83c\udfc1 Performance Results:\")\n",
    "    print(\".4f\")\n",
    "    print(\".4f\")\n",
    "    print(\".2f\")\n",
    "    print(\"\\n\ud83d\udca1 Why Spark might be slower on small data:\")\n",
    "    print(\"   - Overhead of distributed processing\")\n",
    "    print(\"   - Data serialization/deserialization\")\n",
    "    print(\"   - But Spark scales much better with big data!\")\n",
    "\n",
    "    # Cleanup\n",
    "    spark.stop()\n",
    "\n",
    "    return pandas_result, pandas_time, spark_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizations\n",
    "\n",
    "Create charts to understand our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df: pd.DataFrame, manual_results: Dict, spark_results):\n",
    "    \"\"\"Create comparison visualizations\"\"\"\n",
    "    print_header(\"Part 5: Visual Analysis\")\n",
    "\n",
    "    # Create comparison plots\n",
    "    if spark_results is None:\n",
    "        # Simplified visualization without Spark comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Plot 1: Manual MapReduce results\n",
    "        classes = sorted(manual_results.keys())\n",
    "        survival_rates = [manual_results[k][1] for k in classes]\n",
    "        axes[0].bar(classes, survival_rates, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7)\n",
    "        axes[0].set_title('Survival Rate by Class\\n(Manual MapReduce)')\n",
    "        axes[0].set_ylabel('Survival Rate')\n",
    "        axes[0].set_xticks(classes)\n",
    "\n",
    "        # Plot 2: Survival by gender and class\n",
    "        gender_class = df.groupby(['Pclass', 'Sex'])['Survived'].mean().unstack()\n",
    "        gender_class.plot(kind='bar', ax=axes[1], alpha=0.7)\n",
    "        axes[1].set_title('Survival Rate by Class and Gender')\n",
    "        axes[1].set_ylabel('Survival Rate')\n",
    "        axes[1].legend(title='Gender')\n",
    "\n",
    "        print(\"\ud83d\udcca Note: Spark comparison skipped due to environment limitations\")\n",
    "    else:\n",
    "        # Full comparison if Spark works\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot 1: Manual MapReduce results\n",
    "        classes = sorted(manual_results.keys())\n",
    "        survival_rates = [manual_results[k][1] for k in classes]\n",
    "        axes[0, 0].bar(classes, survival_rates, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7)\n",
    "        axes[0, 0].set_title('Survival Rate by Class\\n(Manual MapReduce)')\n",
    "        axes[0, 0].set_ylabel('Survival Rate')\n",
    "        axes[0, 0].set_xticks(classes)\n",
    "\n",
    "        # Plot 2: Spark SQL results\n",
    "        spark_rates = spark_results.set_index('Pclass')['survival_rate']\n",
    "        axes[0, 1].bar(spark_rates.index, spark_rates.values, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7)\n",
    "        axes[0, 1].set_title('Survival Rate by Class\\n(Spark SQL)')\n",
    "        axes[0, 1].set_ylabel('Survival Rate')\n",
    "        axes[0, 1].set_xticks(classes)\n",
    "\n",
    "        # Plot 3: Difference between methods\n",
    "        differences = []\n",
    "        for pclass in classes:\n",
    "            manual_rate = manual_results[pclass][1]\n",
    "            spark_rate = spark_results[spark_results['Pclass'] == pclass]['survival_rate'].iloc[0]\n",
    "            differences.append(abs(manual_rate - spark_rate))\n",
    "\n",
    "        axes[1, 0].bar(classes, differences, color='orange', alpha=0.7)\n",
    "        axes[1, 0].set_title('Difference Between Methods')\n",
    "        axes[1, 0].set_ylabel('Absolute Difference')\n",
    "        axes[1, 0].set_xticks(classes)\n",
    "\n",
    "        # Plot 4: Survival by gender and class\n",
    "        gender_class = df.groupby(['Pclass', 'Sex'])['Survived'].mean().unstack()\n",
    "        gender_class.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "        axes[1, 1].set_title('Survival Rate by Class and Gender')\n",
    "        axes[1, 1].set_ylabel('Survival Rate')\n",
    "        axes[1, 1].legend(title='Gender')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('student_hadoop_demo_results.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\ud83d\udcca Visualizations saved as 'student_hadoop_demo_results.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Java Connection\n",
    "\n",
    "See how this connects to your Java MapReduce lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_java_connection():\n",
    "    \"\"\"Show how PySpark concepts map to Java MapReduce\"\"\"\n",
    "    print_header(\"Part 6: Connecting to Java MapReduce\")\n",
    "\n",
    "    print(\"\ud83d\udd17 Mapping PySpark to Java MapReduce:\")\n",
    "    print(\"\"\"\n",
    "Python/PySpark Approach:\n",
    "spark_df.groupBy(\"Pclass\").agg({\"Survived\": \"avg\"})\n",
    "\n",
    "Java MapReduce Equivalent:\n",
    "public static class SurvivalMapper extends Mapper<Object, Text, IntWritable, IntWritable> {\n",
    "    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] fields = value.toString().split(\",\");\n",
    "        int pclass = Integer.parseInt(fields[2]);  // Pclass column\n",
    "        int survived = Integer.parseInt(fields[1]); // Survived column\n",
    "        context.write(new IntWritable(pclass), new IntWritable(survived));\n",
    "    }\n",
    "}\n",
    "\n",
    "public static class SurvivalReducer extends Reducer<IntWritable, IntWritable, IntWritable, FloatWritable> {\n",
    "    public void reduce(IntWritable key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "        int sum = 0;\n",
    "        int count = 0;\n",
    "        for (IntWritable val : values) {\n",
    "            sum += val.get();\n",
    "            count++;\n",
    "        }\n",
    "        float avg = (float) sum / count;\n",
    "        context.write(key, new FloatWritable(avg));\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\ud83c\udfaf This shows how your Java lab implements the same concepts!\")\n",
    "    print(\"   - Map phase extracts key-value pairs\")\n",
    "    print(\"   - Shuffle phase groups by key\")\n",
    "    print(\"   - Reduce phase aggregates values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "1. Big data needs distributed processing\n",
    "2. MapReduce: map \u2192 shuffle \u2192 reduce\n",
    "3. Spark simplifies distributed computing\n",
    "4. Connects to your Java lab concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main demo function\"\"\"\n",
    "    print(\"\ud83d\ude80 Hadoop & MapReduce Student Hands-On Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Module 10: Scalable Computing - From Single Machine to Distributed Processing\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check environment\n",
    "    if not check_environment():\n",
    "        print(\"\u274c Environment check failed. Please install required packages.\")\n",
    "        return\n",
    "\n",
    "    # Load data\n",
    "    df = load_titanic_data()\n",
    "\n",
    "    # Part 1: Single machine limits\n",
    "    demonstrate_single_machine_limits(df)\n",
    "\n",
    "    # Part 2: Manual MapReduce\n",
    "    manual_results = manual_mapreduce_demo(df)\n",
    "\n",
    "    # Part 3: PySpark demo\n",
    "    print(\"\\n\u26a0\ufe0f  Note: PySpark may not work in all environments.\")\n",
    "    print(\"   We'll demonstrate the concepts conceptually instead.\")\n",
    "    spark_results = None  # Skip PySpark for now\n",
    "    # spark_results = pyspark_demo(df, manual_results)\n",
    "\n",
    "    # Part 4: Performance comparison (simplified without Spark)\n",
    "    print_header(\"Part 4: Performance Analysis\")\n",
    "    print(\"\ud83d\udc3c Testing Pandas Performance...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    pandas_result = df.groupby('Pclass')['Survived'].agg(['mean', 'count', 'sum'])\n",
    "    pandas_time = time.time() - start_time\n",
    "    print(\".4f\")\n",
    "    print(\"\\n\u26a1 Spark comparison skipped due to environment limitations\")\n",
    "    print(\"   In real Hadoop/Spark clusters, Spark would be much faster on big data!\")\n",
    "\n",
    "    # Part 5: Visualizations\n",
    "    create_visualizations(df, manual_results, None)  # Skip Spark results\n",
    "\n",
    "    # Part 6: Java connection\n",
    "    demonstrate_java_connection()\n",
    "\n",
    "    # Summary\n",
    "    print_header(\"Summary & Key Takeaways\")\n",
    "\n",
    "    print(\"\ud83c\udfaf What We Learned Today:\")\n",
    "    print(\"1. The Big Data Problem: Single machines can't handle massive datasets\")\n",
    "    print(\"2. MapReduce Paradigm: map \u2192 shuffle \u2192 reduce pattern\")\n",
    "    print(\"3. PySpark vs Manual: Spark handles complexity automatically\")\n",
    "    print(\"4. Performance Trade-offs: Different tools for different scales\")\n",
    "    print(\"5. Real Hadoop Connection: PySpark concepts map to Java MapReduce\")\n",
    "\n",
    "    print(\"\\n\ud83c\udfc1 Demo completed successfully!\")\n",
    "    print(\"\ud83d\udca1 Next: Try this with larger datasets to see Spark's real power!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}