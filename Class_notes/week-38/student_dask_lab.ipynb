#!/usr/bin/env python3

"""

Dask Student Hands-On Lab Script

Module 13: Scalable Computing - Dask and UCX

This script provides hands-on exercises for students to learn Dask concepts

using Python, bridging the gap between the Java lab and practical data science workflows.

Usage:

    python student_dask_lab.py

Prerequisites:

    pip install dask pandas numpy matplotlib seaborn dask[dataframe] dask[array] dask[distributed]

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import sys
from typing import List, Tuple, Dict, Any

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")

def print_header(title: str):
    """Print a formatted header"""
    print(f"\n{'='*60}")
    print(f"🚀 {title}")
    print(f"{'='*60}")

def check_environment():
    """Check if all required packages are available"""
    print_header("Environment Check")

    # Check Python version
    print(f"Python version: {sys.version}")

    # Test basic imports
    try:
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        print("SUCCESS: Core data science libraries imported successfully")
    except ImportError as e:
        print(f"ERROR: Import error: {e}")
        print("Run: pip install pandas numpy matplotlib seaborn")
        return False

    # Test Dask
    try:
        import dask
        import dask.dataframe as dd
        import dask.array as da
        from dask.distributed import Client
        print("SUCCESS: Dask imported successfully")
        print(f"   Dask version: {dask.__version__}")
    except ImportError as e:
        print(f"ERROR: Dask import error: {e}")
        print("Run: pip install dask[dataframe] dask[array] dask[distributed]")
        return False

    print("\nOBJECTIVE: Ready to start the Dask lab!")
    return True

def load_titanic_data() -> pd.DataFrame:
    """Load Titanic dataset with fallback to generated data"""
    try:
        df = pd.read_csv('titanic.csv')
        print(f"SUCCESS: Titanic dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        print(".2f")
        return df
    except FileNotFoundError:
        print("ERROR: titanic.csv not found. Download it first:")
        print("curl -o titanic.csv https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")
        print("📝 Using generated sample data for demo")

        # Create sample data for demo
        np.random.seed(42)
        df = pd.DataFrame({
            'PassengerId': range(1, 892),
            'Survived': np.random.choice([0, 1], 891),
            'Pclass': np.random.choice([1, 2, 3], 891),
            'Name': [f'Passenger_{i}' for i in range(891)],
            'Sex': np.random.choice(['male', 'female'], 891),
            'Age': np.random.normal(30, 15, 891).clip(0, 80),
            'Fare': np.random.exponential(30, 891)
        })
        return df

## 🐼 Lab 1: Understanding Dask DataFrames

### Exercise 1.1: Basic Dask DataFrame Operations

**Objective:** Learn basic Dask DataFrame operations and lazy evaluation

**Instructions:**
1. Create a Dask DataFrame from pandas DataFrame
2. Perform basic operations (lazy evaluation)
3. Trigger computation and compare with pandas
4. Understand partitioning concepts
def exercise_1_1_basic_operations(df: pd.DataFrame):
    """Exercise: Basic Dask DataFrame operations"""
    print_header("Lab 1: Basic Dask DataFrame Operations")

    import dask.dataframe as dd

    # Exercise 1.1a: Create Dask DataFrame
    print("
📝 Exercise 1.1a: Create Dask DataFrame"    dask_df = dd.from_pandas(df, npartitions=4)
    print(f"SUCCESS: Created Dask DataFrame with {dask_df.npartitions} partitions")
    print(f"   Shape: {dask_df.shape}")
    print(f"   Columns: {list(dask_df.columns)}")

    # Exercise 1.1b: Lazy operations
    print("
📝 Exercise 1.1b: Lazy Operations"    # These operations are lazy - they don't execute immediately
    survival_rate = dask_df['Survived'].mean()
    survival_by_class = dask_df.groupby('Pclass')['Survived'].mean()
    print("   Created lazy operations:")
    print("   - survival_rate = dask_df['Survived'].mean()")
    print("   - survival_by_class = dask_df.groupby('Pclass')['Survived'].mean()")
    print("   (No computation has happened yet!)")

    # Exercise 1.1c: Trigger computation
    print("
📝 Exercise 1.1c: Trigger Computation"    print("   Computing survival rate...")
    computed_survival = survival_rate.compute()
    print(".1%")

    print("   Computing survival by class...")
    computed_by_class = survival_by_class.compute()
    print("   Results:")
    for pclass, rate in computed_by_class.items():
        print(".1%")

    # Exercise 1.1d: Compare with pandas
    print("
📝 Exercise 1.1d: Compare with Pandas"    # Pandas version
    pandas_result = df.groupby('Pclass')['Survived'].mean()

    print("   Pandas results:")
    for pclass, rate in pandas_result.items():
        print(".1%")

    print("   Dask results:")
    for pclass, rate in computed_by_class.items():
        print(".1%")

    # Check if results match
    results_match = np.allclose(pandas_result.values, computed_by_class.values)
    print(f"   Results match: {results_match}")

## 🔢 Lab 2: Dask Array Operations

### Exercise 2.1: Array Creation and Operations

**Objective:** Learn Dask array creation and basic operations

**Instructions:**
1. Create Dask arrays from NumPy arrays
2. Perform mathematical operations
3. Understand chunking and partitioning
4. Compare performance with NumPy
def exercise_2_1_array_operations():
    """Exercise: Dask array operations"""
    print_header("Lab 2: Dask Array Operations")

    import dask.array as da

    # Exercise 2.1a: Create Dask arrays
    print("
📝 Exercise 2.1a: Create Dask Arrays"    # Create large NumPy array
    numpy_array = np.random.random((1000, 1000))
    print(f"   NumPy array shape: {numpy_array.shape}")
    print(".2f")

    # Convert to Dask array
    dask_array = da.from_array(numpy_array, chunks=(250, 250))
    print(f"   Dask array shape: {dask_array.shape}")
    print(f"   Chunk size: {dask_array.chunksize}")
    print(f"   Number of chunks: {dask_array.npartitions}")

    # Exercise 2.1b: Array operations
    print("
📝 Exercise 2.1b: Array Operations"    # Mathematical operations
    squared = dask_array ** 2
    mean_val = dask_array.mean()
    std_val = dask_array.std()

    print("   Created operations (lazy):")
    print("   - squared = dask_array ** 2")
    print("   - mean_val = dask_array.mean()")
    print("   - std_val = dask_array.std()")

    # Compute results
    print("
   Computing results..."    computed_mean = mean_val.compute()
    computed_std = std_val.compute()
    print(".4f")
    print(".4f")

    # Exercise 2.1c: Performance comparison
    print("
📝 Exercise 2.1c: Performance Comparison"    # NumPy performance
    start_time = time.time()
    numpy_mean = numpy_array.mean()
    numpy_std = numpy_array.std()
    numpy_time = time.time() - start_time

    # Dask performance
    start_time = time.time()
    dask_mean = dask_array.mean().compute()
    dask_std = dask_array.std().compute()
    dask_time = time.time() - start_time

    print(".4f")
    print(".4f")
    print(".2f")

## ANALYSIS: Lab 3: Advanced DataFrame Operations

### Exercise 3.1: Complex DataFrame Analysis

**Objective:** Perform complex data analysis with Dask DataFrames

**Instructions:**
1. Filter and transform data
2. Perform multi-level aggregations
3. Handle missing data
4. Create derived columns
def exercise_3_1_complex_analysis(df: pd.DataFrame):
    """Exercise: Complex DataFrame analysis"""
    print_header("Lab 3: Complex DataFrame Analysis")

    import dask.dataframe as dd

    dask_df = dd.from_pandas(df, npartitions=4)

    # Exercise 3.1a: Filtering and transformation
    print("
📝 Exercise 3.1a: Filtering and Transformation"    # Filter adults and create age groups
    adults = dask_df[dask_df['Age'] >= 18]
    adults['age_group'] = adults['Age'].map_partitions(
        lambda x: pd.cut(x, bins=[0, 25, 35, 50, 100],
                        labels=['18-24', '25-34', '35-49', '50+'])
    )

    print("   Created adult passengers DataFrame with age groups")
    print(f"   Adults count: {len(adults)}")

    # Exercise 3.1b: Multi-level aggregation
    print("
📝 Exercise 3.1b: Multi-level Aggregation"    survival_analysis = adults.groupby(['Pclass', 'age_group', 'Sex'])['Survived'].agg(['mean', 'count'])
    results = survival_analysis.compute()

    print("   Survival analysis by class, age group, and gender:")
    print(results.round(3))

    # Exercise 3.1c: Handle missing data
    print("
📝 Exercise 3.1c: Handle Missing Data"    # Fill missing ages with median
    filled_df = dask_df.fillna({'Age': dask_df['Age'].quantile(0.5).compute()})

    # Check missing data
    missing_before = dask_df.isnull().sum().compute()
    missing_after = filled_df.isnull().sum().compute()

    print("   Missing data before filling:")
    print(missing_before[missing_before > 0])

    print("   Missing data after filling:")
    print(missing_after[missing_after > 0])

## 🌐 Lab 4: Distributed Computing

### Exercise 4.1: Local Cluster Operations

**Objective:** Learn distributed computing with Dask LocalCluster

**Instructions:**
1. Start a local Dask cluster
2. Submit tasks to the cluster
3. Monitor cluster performance
4. Scale operations across workers
def exercise_4_1_distributed_computing(df: pd.DataFrame):
    """Exercise: Distributed computing with LocalCluster"""
    print_header("Lab 4: Distributed Computing")

    from dask.distributed import Client, LocalCluster
    import dask.dataframe as dd

    try:
        # Exercise 4.1a: Start local cluster
        print("
📝 Exercise 4.1a: Start Local Cluster"        cluster = LocalCluster(
            n_workers=2,
            threads_per_worker=1,
            memory_limit='512MB',
            dashboard_address=':8788'
        )
        client = Client(cluster)
        print("SUCCESS: Local cluster started")
        print(f"   Dashboard: http://localhost:8788/status")
        print(f"   Workers: {len(cluster.workers)}")

        # Exercise 4.1b: Submit tasks
        print("
📝 Exercise 4.1b: Submit Tasks"        def analyze_passenger_class(df, pclass):
            """Analyze a specific passenger class"""
            class_data = df[df['Pclass'] == pclass]
            return {
                'class': pclass,
                'survival_rate': class_data['Survived'].mean(),
                'avg_fare': class_data['Fare'].mean(),
                'passenger_count': len(class_data)
            }

        # Submit analysis for each class
        futures = []
        for pclass in [1, 2, 3]:
            future = client.submit(analyze_passenger_class, df, pclass)
            futures.append(future)

        # Collect results
        print("   Distributed analysis results:")
        for future in futures:
            result = future.result()
            print(f"   Class {result['class']}: {result['survival_rate']:.1%} survival, "
                  f"${result['avg_fare']:.2f} avg fare, {result['passenger_count']} passengers")

        # Exercise 4.1c: DataFrame operations on cluster
        print("
📝 Exercise 4.1c: Distributed DataFrame Operations"        dask_df = dd.from_pandas(df, npartitions=8)

        # Complex distributed computation
        start_time = time.time()
        result = (
            dask_df[dask_df['Age'] > 18]
            .groupby(['Pclass', 'Sex'])['Survived']
            .mean()
            .compute()
        )
        distributed_time = time.time() - start_time

        print(".4f")
        print("   Results:")
        print(result.round(3))

        client.close()
        cluster.close()
        print("\n🧹 Cluster shut down")

    except Exception as e:
        print(f"ERROR: Distributed exercise failed: {e}")
        print("💡 This may be due to environment limitations.")

## 📈 Lab 5: Performance Optimization

### Exercise 5.1: Optimizing Dask Operations

**Objective:** Learn performance optimization techniques

**Instructions:**
1. Experiment with different partition sizes
2. Use appropriate chunking strategies
3. Implement caching for repeated operations
4. Profile and optimize performance
def exercise_5_1_performance_optimization(df: pd.DataFrame):
    """Exercise: Performance optimization techniques"""
    print_header("Lab 5: Performance Optimization")

    import dask.dataframe as dd

    # Exercise 5.1a: Partition size impact
    print("
📝 Exercise 5.1a: Partition Size Impact"    partition_sizes = [1, 2, 4, 8, 16]

    print("   Testing different partition sizes:")
    for n_part in partition_sizes:
        dask_df = dd.from_pandas(df, npartitions=n_part)

        start_time = time.time()
        result = dask_df.groupby('Pclass')['Survived'].mean().compute()
        elapsed = time.time() - start_time

        print(".4f")

    # Exercise 5.1b: Caching benefits
    print("
📝 Exercise 5.1b: Caching Benefits"    dask_df = dd.from_pandas(df, npartitions=4)

    # Without caching
    start_time = time.time()
    result1 = dask_df[dask_df['Age'] > 25]['Survived'].mean().compute()
    result2 = dask_df[dask_df['Age'] > 25]['Fare'].mean().compute()
    no_cache_time = time.time() - start_time

    # With caching
    dask_df = dd.from_pandas(df, npartitions=4)
    filtered_df = dask_df[dask_df['Age'] > 25]
    filtered_df = filtered_df.persist()  # Cache the filtered data

    start_time = time.time()
    result3 = filtered_df['Survived'].mean().compute()
    result4 = filtered_df['Fare'].mean().compute()
    cache_time = time.time() - start_time

    print(".4f")
    print(".4f")
    print(".2f")

## 🔍 Lab 6: Real-World Application

### Exercise 6.1: E-commerce Analytics

**Objective:** Apply Dask to a real-world analytics scenario

**Scenario:** Analyze e-commerce user behavior data

**Instructions:**
1. Create simulated e-commerce data
2. Perform user behavior analysis
3. Calculate conversion metrics
4. Generate business insights
def exercise_6_1_ecommerce_analytics():
    """Exercise: Real-world e-commerce analytics"""
    print_header("Lab 6: E-commerce Analytics")

    import dask.dataframe as dd

    # Create simulated e-commerce data
    np.random.seed(42)
    n_users = 10000
    n_events = 50000

    users = pd.DataFrame({
        'user_id': range(1, n_users + 1),
        'age': np.random.normal(35, 10, n_users).clip(18, 80),
        'location': np.random.choice(['US', 'EU', 'Asia', 'Other'], n_users)
    })

    events = pd.DataFrame({
        'user_id': np.random.choice(users['user_id'], n_events),
        'event_type': np.random.choice(['view', 'add_to_cart', 'purchase'], n_events,
                                      p=[0.7, 0.2, 0.1]),
        'product_category': np.random.choice(['electronics', 'clothing', 'books', 'home'], n_events),
        'price': np.random.exponential(50, n_events),
        'timestamp': pd.date_range('2024-01-01', periods=n_events, freq='1min')
    })

    # Convert to Dask
    users_dask = dd.from_pandas(users, npartitions=4)
    events_dask = dd.from_pandas(events, npartitions=8)

    # Exercise 6.1a: User behavior analysis
    print("
📝 Exercise 6.1a: User Behavior Analysis"    user_behavior = (
        events_dask.groupby('user_id')
        .agg({
            'event_type': 'count',
            'price': 'sum'
        })
        .compute()
    )

    print("   User behavior summary:")
    print(f"   Total users: {len(user_behavior)}")
    print(".1f")
    print(".2f")

    # Exercise 6.1b: Conversion analysis
    print("
📝 Exercise 6.1b: Conversion Analysis"    conversion_analysis = (
        events_dask.groupby(['user_id', 'event_type'])
        .size()
        .unstack(fill_value=0)
        .compute()
    )

    # Calculate conversion rates
    total_users = len(conversion_analysis)
    purchasers = len(conversion_analysis[conversion_analysis['purchase'] > 0])
    conversion_rate = purchasers / total_users

    print(".1%")
    print(f"   Average purchases per buyer: {conversion_analysis['purchase'].mean():.2f}")

    # Exercise 6.1c: Category performance
    print("
📝 Exercise 6.1c: Category Performance"    category_performance = (
        events_dask[events_dask['event_type'] == 'purchase']
        .groupby('product_category')
        .agg({
            'price': ['sum', 'mean', 'count'],
            'user_id': 'nunique'
        })
        .compute()
    )

    print("   Category performance:")
    print(category_performance.round(2))

## ANALYSIS: Lab 7: Visualization and Reporting

### Exercise 7.1: Creating Visual Reports

**Objective:** Create visualizations and reports from Dask computations

**Instructions:**
1. Create various plots from Dask results
2. Generate summary reports
3. Export results for further analysis
def exercise_7_1_visualization_reporting(df: pd.DataFrame):
    """Exercise: Visualization and reporting"""
    print_header("Lab 7: Visualization and Reporting")

    import dask.dataframe as dd

    dask_df = dd.from_pandas(df, npartitions=4)

    # Exercise 7.1a: Survival analysis visualization
    print("
📝 Exercise 7.1a: Survival Analysis Visualization"    survival_stats = dask_df.groupby('Pclass')['Survived'].agg(['mean', 'count']).compute()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Survival rates
    survival_stats['mean'].plot(kind='bar', ax=ax1, color='skyblue')
    ax1.set_title('Survival Rate by Class')
    ax1.set_ylabel('Survival Rate')
    ax1.set_xlabel('Passenger Class')

    # Passenger counts
    survival_stats['count'].plot(kind='bar', ax=ax2, color='lightgreen')
    ax2.set_title('Passenger Count by Class')
    ax2.set_ylabel('Count')
    ax2.set_xlabel('Passenger Class')

    plt.tight_layout()
    plt.savefig('dask_lab_survival_analysis.png', dpi=150, bbox_inches='tight')
    print("   SUCCESS: Saved survival analysis plot")

    # Exercise 7.1b: Age distribution analysis
    print("
📝 Exercise 7.1b: Age Distribution Analysis"    age_stats = dask_df.groupby('Pclass')['Age'].agg(['mean', 'std', 'min', 'max']).compute()

    plt.figure(figsize=(10, 6))
    age_stats['mean'].plot(kind='bar', yerr=age_stats['std'], capsize=5, color='orange')
    plt.title('Age Distribution by Passenger Class')
    plt.ylabel('Age (years)')
    plt.xlabel('Passenger Class')
    plt.savefig('dask_lab_age_distribution.png', dpi=150, bbox_inches='tight')
    print("   SUCCESS: Saved age distribution plot")

    # Exercise 7.1c: Generate summary report
    print("
📝 Exercise 7.1c: Generate Summary Report"    summary_stats = {
        'total_passengers': len(dask_df),
        'survival_rate': dask_df['Survived'].mean().compute(),
        'avg_age': dask_df['Age'].mean().compute(),
        'avg_fare': dask_df['Fare'].mean().compute(),
        'oldest_passenger': dask_df['Age'].max().compute(),
        'youngest_passenger': dask_df['Age'].min().compute()
    }

    print("   ANALYSIS: Titanic Dataset Summary Report")
    print("   =================================")
    for key, value in summary_stats.items():
        if 'rate' in key:
            print(".1%")
        elif isinstance(value, float):
            print(".2f")
        else:
            print(f"   {key.replace('_', ' ').title()}: {value}")

    plt.show()

def main():
    """Main lab function"""
    print("🚀 Dask Student Hands-On Lab")
    print("=" * 50)
    print("Module 13: Scalable Computing - Dask and UCX")
    print("=" * 50)

    # Check environment
    if not check_environment():
        print("ERROR: Environment check failed. Please install required packages.")
        return

    # Load data
    df = load_titanic_data()

    # Lab exercises
    exercise_1_1_basic_operations(df)
    exercise_2_1_array_operations()
    exercise_3_1_complex_analysis(df)
    exercise_4_1_distributed_computing(df)
    exercise_5_1_performance_optimization(df)
    exercise_6_1_ecommerce_analytics()
    exercise_7_1_visualization_reporting(df)

    # Summary
    print_header("Lab Summary")

    print("OBJECTIVE: What You Learned:")
    print("1. Dask DataFrames: Parallel pandas operations")
    print("2. Dask Arrays: Parallel NumPy operations")
    print("3. Distributed Computing: Scaling to clusters")
    print("4. Performance Optimization: Partitioning and caching")
    print("5. Real-world Applications: E-commerce analytics")
    print("6. Visualization: Creating reports from big data")

    print("
📚 Next Steps:"    print("• Try Dask with larger datasets")
    print("• Explore Dask dashboard monitoring")
    print("• Learn about Dask deployment options")
    print("• Experiment with Dask-ML")
    print("• Integrate with GPU computing")

    print("\n🏁 Lab completed! Great work on learning Dask!")

if __name__ == "__main__":
    main()