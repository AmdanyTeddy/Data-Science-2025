# -*- coding: utf-8 -*-
"""Lab 24 Instruction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PtSs041xZJrmxvGw2zq-771oUpB9tQNQ

# Introduction to Latent Semantic Indexing for Text via Singular Value Decomposition

## cuML + Latent Semantic Indexing (LSI)

Latent semantic indexing (LSI) is the process of extracting and analyzing documents in order to create a representation that captures the similarity of words and documents. LSI assumes that words with similar meaning will occur under similar context, and therefore, the process of LSI utilizes Singular Value Decomposition (SVD) to reduce the dimensionality of the entire word usage representation of all the documents. This allows for documents that are semantically similar but utilize different words to be re-represented as more similar in the reduced vector space. We recommend the following [video introduction of latent semantic indexing (LSI)](https://youtu.be/M1duqgg8-IM) and [Singular Value Decomposition (SVD)](https://youtu.be/g8KZUek79tA).

[cuML](https://docs.rapids.ai/api/cuml/stable/) is RAPIDS' suite of GPU-accelerated machine learning algorithms that mirror's sklearn's API. The entire suite of what cuML offers can be found [here](https://docs.rapids.ai/api/cuml/stable/api.html).
"""

import pandas as pd

import numpy as np

import cudf

import matplotlib.pyplot as plt

from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_extraction.text import TfidfVectorizer

import cuml 
from cuml.decomposition import TruncatedSVD

"""Let's first import the data we will be using for this lab. We will be using sklearn's 20newsgroups dataset. You can read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html). When fetching the training and testing data, make sure to remove headers, footers, and quotes."""

# Fill in the two lines below
newsgroups_train = 
newsgroups_test =

"""Let's see what categories the newsgroup documents fall under."""

newsgroups_train.target_names

"""Now we have to turn our newsgroup documents into a representation that we can run SVD on. For that, we can utilize TFIDF, or term frequency-inverse document frequency. TFIDF extracts information from a document through counting the number of occurrences of each word in the document and then scaling down the impact of words that appear very frequently. The scaling prevents words that appear very frequently and often times provide little information, such as "and", "so", and "the", from becoming significant words in a document. More of TFIDF can be read [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). 

We will use cuML's TFIDF vectorizer. The API for TDIDF can be found within [cuML's](https://docs.rapids.ai/api/cuml/stable/api.html) documentation. Fit the vectorizer with the training data and transform both the training and testing data. Make sure to convert the transformed data into numpy arrays.
"""

vectorizer = TfidfVectorizer(strip_accents='ascii')
# Fill in the two lines below
training_data = 
testing_data = 

print("training data shape: ", training_data.shape)
print("testing data shape: ", testing_data.shape)

"""Display our test data as a pandas dataframe. Don't forget to add the column name for pandas."""

pdf = pd.DataFrame(training_data, columns = vectorizer.get_feature_names())
pdf

"""There are quite a few features in our bag of words, more than 100 thousand! We will cut it down a bit to help our runtime in the following steps using sklearn's SelectPercentile function. [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile) selects the top features of a dataset which allows us to discard features which are not as important. Be sure to fit the model on our training data and the training data's target, and don't forget to transform needed datasets. """

selector = SelectPercentile(percentile = 10)

# Fit and transform the training data
training_data =

"""After applying our SelectPercentile function, we have removed 90% of the initial features. The column headers will need to be updated to reflect the selected features (hint: look at the methods available to SelectPercentile)."""

# Identify the labels of the selected features

new_features =
print(new_features)

"""Let's take a look at our new data. """

updated_pdf = pd.DataFrame(training_data, columns = new_features)
updated_pdf

"""In order to run SVD on our vectorized training data, we will need to use cuML's TruncatedSVD API. Set the number of components to be 100, the number of iterations to be 25, and the algorithm to be Jacobi. Then fit the model with the training dataset and then transform the training dataset. Documentation on cuML's TruncatedSVD can be found [here](https://docs.rapids.ai/api/cuml/stable/api.html)."""

# Set lsi to the TruncatedSVD model
lsi = 

cuml.set_global_output_type('numpy')

# Fill in the line below
transformed_training =

"""After fitting our SVD model, let's visualize our singular values. Plot the singular values in descending order."""

# Create the plot. It should resemble the plot below.


plt.xlabel("Singular value number")
plt.ylabel("Singular value")
plt.show()

"""Let's take a look at what words are associated with each component. Let us use the 4th most significant component for this example (index 3). Remember that the larger the component's associated singular value, the more significant the component, and the words associated with each component can be found in the components_ of TruncatedSVD. Print the top 25 words in the 4th component. Finding the top words for each component can be achieved through sorting the component from greatest to least and identifying the ordering of the features in the sorted arrangement (hint, use argsort). """

# Fill in the two lines below
component = 
V_T = 

print(V_T.shape)

# Find the associated words for the given component. After ranking them, print the top 25 words for the component. You should 
# get the words below in that order.

"""Now let take the 501st datapoint (index 500) from our testing dataset. We will set this as our "search" document. We will find the document in the training set that is related closest to this document, but first, let us take a look at the content of this document and its classification."""

# Display the document and its classification. The content below is the correct output.

"""We will now reduce then transform our search document into its component form to compare it with our reduced and transformed training set. You can reduce the datapoint to fit our training set by transforming with our previous SelectPercentile model. You will then need to transform the new document into a representation we can use to compare to the existing documents. Because our training documents have been transformed from containing features to containing components, we will need convert our search document to have the same representation. We can do this by performing a dot product on the TFIDF representation of our search document with our SVD components (be sure the dimensions match)."""

# Reduce the document's features to match our LSI model, then transform the document from feature format to component format

"""To find the document in our training set most similar to our choosen document. We will need to run a [cosine similarity function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) between our transformed search document and the transformed training set. Once you have the cosine similarity values, order the training set documents from most to least similar to our search document."""

# Find the most similar documents to our selected document using cosine similarity. The numbers below should be your most 
# to least similar document ordering.

"""What are the contents of the top 3 most similar documents in the training set? What categories do they fall under? Does they look similar to our initial search document?"""

# Display the content/data of the top 3 documents.