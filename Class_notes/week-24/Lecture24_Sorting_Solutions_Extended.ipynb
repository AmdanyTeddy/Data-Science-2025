{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bd77cb",
   "metadata": {},
   "source": [
    "# üöÄ Lecture 24 ‚Äî Advanced: Modern 2025 Sorting Techniques\n",
    "Extension for Instructor Notebook ‚Äî Parallel, GPU, External-Memory & Distributed Sorting\n",
    "---\n",
    "This section introduces modern approaches used in 2025 production systems: parallel CPU sorting, GPU-accelerated sorting, external-memory (out-of-core) sorting, and distributed sorting frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6f6d8",
   "metadata": {},
   "source": [
    "## üîç Overview & When to use which\n",
    "\n",
    "- **In-memory sorts (Œò(n log n))**: Use for data that fits in RAM; e.g., `sorted()`, `numpy.sort()`.\n",
    "- **Parallel CPU sorting**: Use multi-core machines to sort large arrays faster by dividing work across cores.\n",
    "- **GPU sorting**: Use when sorting very large numeric arrays where GPU memory and throughput provide huge speedups (RAPIDS / cuDF / Thrust / CUDA radix sort).\n",
    "- **External-memory (out-of-core) sorting**: For datasets larger than RAM; use chunking + external merge.\n",
    "- **Distributed sorting (Spark/Hadoop)**: For petabyte-scale data across many machines; use distributed shuffle and sort-by-key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7cf122",
   "metadata": {},
   "source": [
    "## 1) Parallel CPU Sorting (chunk, sort, merge)\n",
    "\n",
    "Idea: split the array into `k` chunks, sort each chunk in parallel (multi-thread/process), then merge the sorted chunks with a k-way merge.\n",
    "Complexity: work remains Œò(n log n), but wall-clock time can approach Œò((n log n)/p) given p workers (plus merging overhead).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02469c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Parallel sort using concurrent.futures (works in Colab)\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import heapq\n",
    "import math\n",
    "import random\n",
    "\n",
    "def sort_chunk(chunk):\n",
    "    return sorted(chunk)\n",
    "\n",
    "def parallel_sort(arr, n_workers=4):\n",
    "    # split into roughly equal chunks\n",
    "    n = len(arr)\n",
    "    if n_workers <= 1 or n < 2_000:\n",
    "        return sorted(arr)\n",
    "    chunk_size = math.ceil(n / n_workers)\n",
    "    chunks = [arr[i:i+chunk_size] for i in range(0, n, chunk_size)]\n",
    "    sorted_chunks = []\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "        futures = [ex.submit(sort_chunk, c) for c in chunks]\n",
    "        for fut in as_completed(futures):\n",
    "            sorted_chunks.append(fut.result())\n",
    "    # k-way merge using heapq.merge (lazy generator)\n",
    "    return list(heapq.merge(*sorted_chunks))\n",
    "\n",
    "# demo with random data\n",
    "arr = [random.random() for _ in range(10000)]\n",
    "res = parallel_sort(arr, n_workers=4)\n",
    "print(len(res), res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62ace1",
   "metadata": {},
   "source": [
    "## 2) GPU-Accelerated Sorting (RAPIDS / cuDF / CuPy)\n",
    "\n",
    "GPUs can sort large numeric arrays much faster using massively parallel primitives. In Python, RAPIDS (`cudf`) and `cupy` mirror `pandas`/`numpy` APIs.\n",
    "\n",
    "- Typical approach: move data to GPU memory, call GPU sort (radix or merge-based), retrieve results.\n",
    "- Complexity: Œò(n log n) or Œò(n) for specialized radix sorts on integers; massive constant-factor speedups for throughput.\n",
    "\n",
    "Colab note: GPU runtimes may not have RAPIDS preinstalled; the examples below are guarded and optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae107b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional GPU example (will run only if cupy is installed and GPU is available)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import numpy as np\n",
    "    a = cp.asarray(np.random.rand(10_000_00))  # 1M elements if memory allows\n",
    "    # cupy.sort uses GPU\n",
    "    t0 = cp.cuda.Event(); t1 = cp.cuda.Event()\n",
    "    t0.record()\n",
    "    a_sorted = cp.sort(a)\n",
    "    t1.record(); t1.synchronize()\n",
    "    print(\"GPU sort done. Time (approx ms):\", cp.get_default_memory_pool())\n",
    "    # bring back a small slice\n",
    "    print(cp.asnumpy(a_sorted[:5]))\n",
    "except Exception as e:\n",
    "    print(\"GPU sort example skipped (cupy not available or GPU not present):\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093aa11",
   "metadata": {},
   "source": [
    "## 3) External-Memory (Out-of-Core) Sorting\n",
    "\n",
    "When data doesn't fit in RAM, use external merge sort:\n",
    "1. Read chunks that fit in memory, sort each chunk, write sorted chunk to disk.\n",
    "2. Perform a k-way merge of the chunk files (use priority queue) to produce final sorted output.\n",
    "\n",
    "Complexity: Œò(n log n) I/O and CPU; dominated by disk I/O cost. Key metrics: number of passes, read/write volume, sequential I/O bandwidth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# External merge sort (sketch). This demo uses small in-memory temp lists instead of files.\n",
    "import heapq, tempfile, os\n",
    "\n",
    "def external_sort_sim(data, chunk_size=1000):\n",
    "    # Step 1: write sorted chunks to temp files\n",
    "    temp_files = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = sorted(data[i:i+chunk_size])\n",
    "        tf = tempfile.NamedTemporaryFile(delete=False, mode='w+t')\n",
    "        for x in chunk:\n",
    "            tf.write(f\"{x}\\n\")\n",
    "        tf.flush(); tf.seek(0)\n",
    "        temp_files.append(tf)\n",
    "    # Step 2: k-way merge from temp files\n",
    "    iterators = [map(float, open(tf.name)) for tf in temp_files]\n",
    "    with open('external_sorted_output.txt', 'w') as out:\n",
    "        for val in heapq.merge(*iterators):\n",
    "            out.write(str(val) + '\\n')\n",
    "    # cleanup\n",
    "    for tf in temp_files:\n",
    "        try:\n",
    "            os.unlink(tf.name)\n",
    "        except:\n",
    "            pass\n",
    "    return 'external_sorted_output.txt'\n",
    "\n",
    "# demo\n",
    "data = [random.random() for _ in range(5000)]\n",
    "out_file = external_sort_sim(data, chunk_size=500)\n",
    "print('Wrote sorted output to', out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd0d6d",
   "metadata": {},
   "source": [
    "## 4) Distributed Sorting (Spark / Dask / Hadoop)\n",
    "\n",
    "For huge datasets across machines, distributed frameworks perform a shuffle and sort-by-key. Example tools:\n",
    "- Apache Spark: `rdd.sortByKey()` or `df.sort()` with partitioning.\n",
    "- Dask: parallelize pandas-like operations across cluster.\n",
    "- Hadoop MapReduce: map phase emits key,value and reduce collects sorted keys.\n",
    "\n",
    "Complexity: network and disk I/O dominate; wall-clock time depends on cluster size, data skew, and shuffle costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb83c0d",
   "metadata": {},
   "source": [
    "### Spark example (pseudocode)\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile('hdfs://...')\n",
    "pairs = rdd.map(lambda line: (key_fn(line), line))\n",
    "sorted = pairs.sortByKey()  # distributed shuffle + local sorts\n",
    "sorted.saveAsTextFile('hdfs://.../sorted')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752bc1b",
   "metadata": {},
   "source": [
    "## 5) Specialized / Practical Techniques\n",
    "\n",
    "- **Radix sort** for integers: Œò(n) time with base choice; excellent in practice for fixed-width keys.\n",
    "- **Parallel k-way merge algorithms** to merge many sorted runs efficiently.\n",
    "- **External-memory libraries**: GNU `sort`, `pandas.read_csv(..., chunksize=...)`, `dask.dataframe`, `vaex`.\n",
    "- **Avoid full sorts when possible**: use `nlargest`, `nsmallest`, streaming top-k with heaps (Œò(n log k)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: top-k streaming using heapq (useful in log processing / ML candidate selection)\n",
    "import heapq, random\n",
    "\n",
    "def top_k_stream(iterable, k=10):\n",
    "    h = []\n",
    "    for x in iterable:\n",
    "        if len(h) < k:\n",
    "            heapq.heappush(h, x)\n",
    "        else:\n",
    "            if x > h[0]:\n",
    "                heapq.heapreplace(h, x)\n",
    "    return sorted(h, reverse=True)\n",
    "\n",
    "data = [random.random() for _ in range(10000)]\n",
    "print('Top 5:', top_k_stream(data, k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a0fdf",
   "metadata": {},
   "source": [
    "## 6) Practical Notes & Tradeoffs\n",
    "\n",
    "- **Memory vs Time**: Merge sort uses extra memory; in-place sorts save RAM.\n",
    "- **IO bottlenecks**: For external sorts, sequential disk throughput matters more than CPU.\n",
    "- **Data skew**: In distributed sorts, skew causes stragglers; use custom partitioning.\n",
    "- **Stability**: Important when sorting by multiple columns; choose stable algorithms when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6632a",
   "metadata": {},
   "source": [
    "## ‚úÖ Hands-on Tasks (Instructor)\n",
    "1. Run the parallel sort example with varying `n_workers` and input sizes.\n",
    "2. If GPU is available, try the CUDA/CuPy example; compare speedups.\n",
    "3. Simulate external sort on a large synthetic dataset and measure I/O time.\n",
    "4. Demonstrate Spark `sortByKey()` on a small cluster or cloud notebook (pseudocode ok).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}