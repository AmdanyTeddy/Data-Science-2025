# .github/workflows/grade_assignments.yml
name: Automated Assignment Grading

on:
  push:
    paths:
      - 'Submissions/**'
  pull_request:
    paths:
      - 'Submissions/**'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      generate_feedback:
        description: 'Generate individual feedback files'
        required: false
        default: 'true'
        type: boolean

jobs:
  grade-assignments:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
      pull-requests: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for better analysis
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install nbformat
        # Add any other required packages
        
    - name: Create grader script
      run: |
        # The grader script should be in your repository
        # Or you can embed it here if preferred
        if [ ! -f "grade_assignments.py" ]; then
          echo "Creating grader script..."
          # You would copy the grader script here or ensure it's in the repo
        fi
        
    - name: Run assignment grading
      run: |
        python grade_assignments.py \
          --repo-path . \
          --output grading_results.json \
          $(if [ "${{ github.event.inputs.generate_feedback }}" = "true" ]; then echo "--generate-feedback"; fi)
    
    - name: Upload grading results
      uses: actions/upload-artifact@v3
      with:
        name: grading-results
        path: |
          grading_results.json
          feedback/
          grader.log
        retention-days: 30
    
    - name: Create summary comment (for PRs)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const results = JSON.parse(fs.readFileSync('grading_results.json', 'utf8'));
            const summary = results._summary || {};
            
            const comment = `## 📊 Assignment Grading Summary
            
**Grading Results:**
- 👥 Students: ${summary.total_students || 0}
- 📝 Submissions: ${summary.total_submissions || 0}
- 📈 Average Grade: ${summary.average_grade || 0}/100
- 🤖 AI Detection Flags: ${summary.ai_flagged_submissions || 0} (${summary.ai_flagged_percentage || 0}%)

**Grade Distribution:**
${Object.entries(summary.grade_distribution || {}).map(([range, count]) => `- ${range}: ${count} students`).join('\n')}

${summary.ai_flagged_submissions > 0 ? '⚠️ **Note:** Some submissions were flagged for possible AI assistance. Please review individual feedback.' : ''}

*Detailed results and feedback files are available in the workflow artifacts.*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.error('Error creating comment:', error);
          }
    
    - name: Commit feedback files (if main branch)
      if: github.ref == 'refs/heads/main' && github.event.inputs.generate_feedback == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if [ -d "feedback" ]; then
          git add feedback/
          git add grading_results.json
          
          if ! git diff --staged --quiet; then
            git commit -m "📊 Update assignment feedback and grading results
            
            - Generated feedback for $(ls feedback/*.md 2>/dev/null | wc -l) students
            - Average grade: $(python -c "import json; data=json.load(open('grading_results.json')); print(data.get('_summary', {}).get('average_grade', 'N/A'))")/100
            - AI detection flags: $(python -c "import json; data=json.load(open('grading_results.json')); print(data.get('_summary', {}).get('ai_flagged_submissions', 0))")
            
            Auto-generated by assignment grader"
            
            git push
          else
            echo "No changes to commit"
          fi
        fi
    
    - name: Generate assignment report
      run: |
        python -c "
import json
import sys
from datetime import datetime

try:
    with open('grading_results.json', 'r') as f:
        results = json.load(f)
    
    summary = results.get('_summary', {})
    
    print('# Assignment Grading Report')
    print(f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')
    print()
    print('## Summary Statistics')
    print(f'- **Total Students:** {summary.get(\"total_students\", 0)}')
    print(f'- **Total Submissions:** {summary.get(\"total_submissions\", 0)}')
    print(f'- **Average Grade:** {summary.get(\"average_grade\", 0)}/100')
    print(f'- **AI Detection Flags:** {summary.get(\"ai_flagged_submissions\", 0)} ({summary.get(\"ai_flagged_percentage\", 0)}%)')
    print()
    
    print('## Grade Distribution')
    grade_dist = summary.get('grade_distribution', {})
    for grade_range, count in grade_dist.items():
        print(f'- **{grade_range}:** {count} students')
    print()
    
    if summary.get('ai_flagged_submissions', 0) > 0:
        print('## ⚠️ AI Detection Alerts')
        print('The following submissions were flagged for possible AI assistance:')
        print()
        
        for student_name, student_data in results.items():
            if student_name == '_summary':
                continue
            
            flagged_submissions = []
            for assignment_type in ['individual', 'group', 'project']:
                if assignment_type in student_data:
                    for submission in student_data[assignment_type].get('submissions', []):
                        ai_likelihood = submission['ai_detection'].get('likelihood', 'Very Low')
                        if ai_likelihood in ['High', 'Very High']:
                            flagged_submissions.append({
                                'file': submission['file_name'],
                                'type': assignment_type,
                                'likelihood': ai_likelihood,
                                'confidence': submission['ai_detection'].get('confidence', 0)
                            })
            
            if flagged_submissions:
                print(f'### {student_name}')
                for sub in flagged_submissions:
                    print(f'- **{sub[\"file\"]}** ({sub[\"type\"]}): {sub[\"likelihood\"]} likelihood ({sub[\"confidence\"]}% confidence)')
                print()
    
    print('## Recommendations')
    avg_grade = summary.get('average_grade', 0)
    if avg_grade >= 85:
        print('- Overall class performance is excellent!')
    elif avg_grade >= 75:
        print('- Good class performance overall. Some students may need additional support.')
    elif avg_grade >= 65:
        print('- Class performance is satisfactory but could be improved.')
    else:
        print('- Class performance needs significant improvement. Consider additional tutorials or office hours.')
    
    if summary.get('ai_flagged_submissions', 0) > 0:
        print('- Address AI assistance concerns with flagged students.')
        print('- Consider discussing academic integrity policies.')
    
except FileNotFoundError:
    print('Grading results file not found.')
    sys.exit(1)
except Exception as e:
    print(f'Error generating report: {e}')
    sys.exit(1)
" > GRADING_REPORT.md
    
    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: grading-report
        path: GRADING_REPORT.md
        retention-days: 90

  notify-instructor:
    needs: grade-assignments
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Download results
      uses: actions/download-artifact@v3
      with:
        name: grading-results
    
    - name: Send notification (optional)
      if: github.ref == 'refs/heads/main'
      run: |
        # Optional: Send email notification or Slack message
        # You can configure this based on your preferences
        
        if [ -f "grading_results.json" ]; then
          echo "Grading completed successfully!"
          echo "Results available in workflow artifacts."
          
          # Example: Send to webhook (replace with your webhook URL)
          # curl -X POST -H 'Content-type: application/json' \
          #   --data '{"text":"Assignment grading completed for Data-Science-2025 repository"}' \
          #   YOUR_WEBHOOK_URL
        else
          echo "Grading failed - no results file found"
        fi

# Additional workflow for scheduled grading (optional)
---
name: Weekly Grading Summary

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:

jobs:
  weekly-summary:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install nbformat
    
    - name: Run weekly grading
      run: |
        python grade_assignments.py \
          --repo-path . \
          --output weekly_grading_results.json \
          --generate-feedback
    
    - name: Generate weekly report
      run: |
        python -c "
import json
import os
from datetime import datetime, timedelta

# Load current results
with open('weekly_grading_results.json', 'r') as f:
    current_results = json.load(f)

summary = current_results.get('_summary', {})

print('# Weekly Grading Summary')
print(f'Week of: {datetime.now().strftime(\"%Y-%m-%d\")}')
print()
print('## Current Status')
print(f'- Active Students: {summary.get(\"total_students\", 0)}')
print(f'- Total Submissions: {summary.get(\"total_submissions\", 0)}')
print(f'- Class Average: {summary.get(\"average_grade\", 0)}/100')
print()

# Identify students needing attention
print('## Students Needing Attention')
low_performers = []
ai_concerns = []

for student_name, student_data in current_results.items():
    if student_name == '_summary':
        continue
    
    # Check for low performance
    all_grades = []
    ai_flags = 0
    
    for assignment_type in ['individual', 'group', 'project']:
        if assignment_type in student_data:
            for submission in student_data[assignment_type].get('submissions', []):
                if isinstance(submission['grade'], (int, float)):
                    all_grades.append(submission['grade'])
                
                if submission['ai_detection'].get('likelihood') in ['High', 'Very High']:
                    ai_flags += 1
    
    if all_grades:
        avg_grade = sum(all_grades) / len(all_grades)
        if avg_grade < 70:
            low_performers.append((student_name, avg_grade))
    
    if ai_flags > 0:
        ai_concerns.append((student_name, ai_flags))

if low_performers:
    print('### Low Performance (< 70% average):')
    for student, grade in sorted(low_performers, key=lambda x: x[1]):
        print(f'- **{student}**: {grade:.1f}%')
    print()

if ai_concerns:
    print('### AI Detection Concerns:')
    for student, flags in sorted(ai_concerns, key=lambda x: x[1], reverse=True):
        print(f'- **{student}**: {flags} flagged submissions')
    print()

print('## Recommendations')
if len(low_performers) > len(current_results) * 0.3:
    print('- High number of struggling students - consider review session')
if ai_concerns:
    print('- Address academic integrity concerns with flagged students')
print('- Review individual feedback files for detailed recommendations')
" > WEEKLY_REPORT.md
    
    - name: Upload weekly report
      uses: actions/upload-artifact@v3
      with:
        name: weekly-report
        path: |
          WEEKLY_REPORT.md
          weekly_grading_results.json
          feedback/
        retention-days: 90